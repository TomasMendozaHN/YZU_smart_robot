{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bound-serum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-2kkdawtw because the default path (/home/jetbot/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n",
      "CUDA available: True\n",
      "cuDNN version: 8000\n"
     ]
    }
   ],
   "source": [
    "run Keypoints_RCNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "optical-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "run 2_Calculating_Angles.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acute-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "run 3_Reading_RPLidar.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "laden-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "run 4_Stage_Estimation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "secure-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    '''\n",
    "    This wrapper works as an RL-Environment for the Jetson Nano.\n",
    "    Arguments are: \n",
    "        model  -> pytorch model trained in Unity.\n",
    "        angles -> list of angles required to measure\n",
    "        FOV    -> How wide the camera lens is. Default = 160\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model, angles, FOV=160):\n",
    "        self.previous_readings = {x:0 for x in angles} \n",
    "        self.angles = angles\n",
    "        self.model = model\n",
    "        self.FOV = FOV\n",
    "        \n",
    "        \n",
    "    def calculate_angle_and_phase(self):\n",
    "        keypoints, image, counts, objects, peaks = execute({'new': camera.value})\n",
    "        angle, keypoints = calculate_angle(WIDTH, keypoints, self.FOV)\n",
    "        phase = self.calculate_phase(keypoints)\n",
    "        return phase, angle\n",
    "    \n",
    "    \n",
    "    def calculate_phase(self, keypoints):\n",
    "        phase = estimating_phase(keypoints)\n",
    "        return phase\n",
    "    \n",
    "    \n",
    "    def read_lidar(self):\n",
    "        self.previous_readings = read_lidar_wrapper(self.angles,self.previous_readings)\n",
    "        \n",
    "    \n",
    "    def observe(self):\n",
    "        self.read_lidar()\n",
    "        phase, angle = self.calculate_angle_and_phase()\n",
    "        \n",
    "        if isinstance(angle, dict):\n",
    "            angles = list(angle.values())\n",
    "        \n",
    "        else: angles = [-1]\n",
    "        \n",
    "        observation = phase + angle + list(self.previous_readings.values())\n",
    "        return observation\n",
    "    \n",
    "    \n",
    "    def sample_action(self, observation):\n",
    "        observation = torch.Tensor(observation).cuda()\n",
    "        hidden,_ = self.model.network_body(vis_inputs=[0],vec_inputs=[observation])\n",
    "        distribution = self.model.distribution(hidden)\n",
    "        action = distribution.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "exciting-needle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeparateActorCritic(\n",
       "  (network_body): NetworkBody(\n",
       "    (visual_processors): ModuleList()\n",
       "    (vector_processors): ModuleList(\n",
       "      (0): VectorInput(\n",
       "        (normalizer): Normalizer()\n",
       "      )\n",
       "    )\n",
       "    (linear_encoder): LinearEncoder(\n",
       "      (seq_layers): Sequential(\n",
       "        (0): Linear(in_features=22, out_features=256, bias=True)\n",
       "        (1): Swish()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Swish()\n",
       "        (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (5): Swish()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (distribution): GaussianDistribution(\n",
       "    (mu): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       "  (critic): ValueNetwork(\n",
       "    (network_body): NetworkBody(\n",
       "      (visual_processors): ModuleList()\n",
       "      (vector_processors): ModuleList(\n",
       "        (0): VectorInput(\n",
       "          (normalizer): Normalizer()\n",
       "        )\n",
       "      )\n",
       "      (linear_encoder): LinearEncoder(\n",
       "        (seq_layers): Sequential(\n",
       "          (0): Linear(in_features=22, out_features=256, bias=True)\n",
       "          (1): Swish()\n",
       "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (3): Swish()\n",
       "          (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (5): Swish()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (value_heads): ValueHeads(\n",
       "      (value_heads): ModuleDict(\n",
       "        (extrinsic): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"MobileRobot-16325.pth\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "stone-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = [x for x in range(0,180,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "studied-mount",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, -1, 568, 560, 1687, 0, 2014, 0, 386, 0, 542, 654, 459, 433, 507, 482, 511, 519, 334, 0]\n"
     ]
    }
   ],
   "source": [
    "env = Environment(model, angles)\n",
    "observation = env.observe()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "premium-camping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2607, -0.3771]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = env.sample_action(observation)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-suspect",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
