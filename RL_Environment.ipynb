{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bound-serum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-rrno3ym1 because the default path (/home/jetbot/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "run Keypoints_RCNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "optical-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "run 2_Calculating_Angles.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acute-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "run 3_Reading_RPLidar.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laden-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "run 4_Stage_Estimation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-benchmark",
   "metadata": {},
   "source": [
    "# Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secure-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    '''\n",
    "    This wrapper works as an RL-Environment for the Jetson Nano.\n",
    "    Arguments are: \n",
    "        model  -> pytorch model trained in Unity.\n",
    "        angles -> list of angles required to measure\n",
    "        FOV    -> How wide the camera lens is. Default = 160\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model, angles, robot, FOV=160):\n",
    "        self.previous_readings = {x:0 for x in angles} \n",
    "        self.angles = angles\n",
    "        self.model = model\n",
    "        self.FOV = FOV\n",
    "        self.robot = robot\n",
    "        \n",
    "        \n",
    "    def calculate_angle_and_phase(self):\n",
    "        keypoints, image, counts, objects, peaks = execute({'new': camera.value})\n",
    "        angle, keypoints = calculate_angle(WIDTH, keypoints, self.FOV)\n",
    "        phase = self.calculate_phase(keypoints)\n",
    "        return phase, angle\n",
    "    \n",
    "    \n",
    "    def step(tensor):\n",
    "        turn, move = tensor.cpu()\n",
    "        \n",
    "    \n",
    "    def stop(self):\n",
    "        self.robot.stop()\n",
    "\n",
    "        \n",
    "    def step_forward(self):\n",
    "        self.robot.forward(0.4)\n",
    "        time.sleep(0.5)\n",
    "        self.robot.stop()\n",
    "\n",
    "        \n",
    "    def step_backward(self):\n",
    "        self.robot.backward(0.4)\n",
    "        time.sleep(0.5)\n",
    "        self.robot.stop()\n",
    "\n",
    "        \n",
    "    def step_left(self):\n",
    "        self.robot.left(0.3)\n",
    "        time.sleep(0.5)\n",
    "        self.robot.stop()\n",
    "\n",
    "        \n",
    "    def step_right(self):\n",
    "        self.robot.right(0.3)\n",
    "        time.sleep(0.5)\n",
    "        self.robot.stop()\n",
    "    \n",
    "    \n",
    "    def calculate_phase(self, keypoints):\n",
    "        phase = estimating_phase(keypoints)\n",
    "        return phase\n",
    "    \n",
    "    \n",
    "    def read_lidar(self):\n",
    "        self.previous_readings = read_lidar_wrapper(self.angles,self.previous_readings)\n",
    "        \n",
    "    \n",
    "    def observe(self):\n",
    "        self.read_lidar()\n",
    "        phase, angle = self.calculate_angle_and_phase()\n",
    "        \n",
    "        if isinstance(angle, dict):\n",
    "            angles = list(angle.values())\n",
    "        \n",
    "        else: angles = [-1]\n",
    "        \n",
    "        observation = phase + angle + list(self.previous_readings.values())\n",
    "        return observation\n",
    "    \n",
    "    \n",
    "    def sample_action(self, observation):\n",
    "        observation = torch.Tensor(observation).cuda()\n",
    "        hidden,_ = self.model.network_body(vis_inputs=[0],vec_inputs=[observation])\n",
    "        distribution = self.model.distribution(hidden)\n",
    "        action = distribution.sample()\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def step(self,action):\n",
    "        action = action.cpu().detach().numpy()[0]\n",
    "        \n",
    "        speed_move, speed_turn = float(abs(action[0])), float(abs(action[1]))\n",
    "        \n",
    "        # backward\n",
    "        if action[0] < 0:\n",
    "            robot.backward(speed_move)\n",
    "            time.sleep(0.5)\n",
    "            robot.stop()\n",
    "            \n",
    "        # forward\n",
    "        else:\n",
    "            robot.forward(speed_move)\n",
    "            time.sleep(0.5)\n",
    "            robot.stop()\n",
    "            \n",
    "        time.sleep(0.2)\n",
    "            \n",
    "        # turn left\n",
    "        if action[1] < 0:\n",
    "            robot.left(speed_turn)\n",
    "            time.sleep(0.3)\n",
    "            robot.stop()\n",
    "            \n",
    "        # turn right\n",
    "        else:\n",
    "            robot.right(speed_turn)\n",
    "            time.sleep(0.3)\n",
    "            robot.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-muscle",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exciting-needle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeparateActorCritic(\n",
       "  (network_body): NetworkBody(\n",
       "    (visual_processors): ModuleList()\n",
       "    (vector_processors): ModuleList(\n",
       "      (0): VectorInput(\n",
       "        (normalizer): Normalizer()\n",
       "      )\n",
       "    )\n",
       "    (linear_encoder): LinearEncoder(\n",
       "      (seq_layers): Sequential(\n",
       "        (0): Linear(in_features=22, out_features=256, bias=True)\n",
       "        (1): Swish()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Swish()\n",
       "        (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (5): Swish()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (distribution): GaussianDistribution(\n",
       "    (mu): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       "  (critic): ValueNetwork(\n",
       "    (network_body): NetworkBody(\n",
       "      (visual_processors): ModuleList()\n",
       "      (vector_processors): ModuleList(\n",
       "        (0): VectorInput(\n",
       "          (normalizer): Normalizer()\n",
       "        )\n",
       "      )\n",
       "      (linear_encoder): LinearEncoder(\n",
       "        (seq_layers): Sequential(\n",
       "          (0): Linear(in_features=22, out_features=256, bias=True)\n",
       "          (1): Swish()\n",
       "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (3): Swish()\n",
       "          (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (5): Swish()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (value_heads): ValueHeads(\n",
       "      (value_heads): ModuleDict(\n",
       "        (extrinsic): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"MobileRobot-16325.pth\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-slide",
   "metadata": {},
   "source": [
    "# Define the angles to read using LiDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stone-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = [x for x in range(0,180,10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-running",
   "metadata": {},
   "source": [
    "# Initialize the robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intellectual-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "studied-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(model, angles, robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "chief-pierre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 53, 0, 0, 0, 0, 0, 0, 295, 0, 0, 903, 0, 479, 609, 793, 619, 589, 0, 219]\n"
     ]
    }
   ],
   "source": [
    "observation = env.observe()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "premium-camping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2417, -0.6389]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = env.sample_action(observation)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lonely-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
